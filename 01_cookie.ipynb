{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Bayes's Theorem\n",
    "\n",
    "[Bayesian Decision Analysis](https://allendowney.github.io/BayesianDecisionAnalysis/)\n",
    "\n",
    "Copyright 2021 Allen B. Downey\n",
    "\n",
    "License: [Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes's Theorem\n",
    "\n",
    "There are two ways to think about Bayes's Theorem:\n",
    "\n",
    "* It is a divide-and conquer strategy for computing conditional probabilities.  If it's hard to compute $P(A|B)$ directly, sometimes it is easier to compute the terms on the other side of the equation: $P(A)$, $P(B|A)$, and $P(B)$.\n",
    "\n",
    "* It is also a recipe for updating beliefs in the light of new data.\n",
    "\n",
    "When we are working with the second interpretation, we often write Bayes's Theorem with different variables.  Instead of $A$ and $B$, we use $H$ and $D$, where\n",
    "\n",
    "* $H$ stands for \"hypothesis\", and\n",
    "\n",
    "* $D$ stands for \"data\".\n",
    "\n",
    "So we write Bayes's Theorem like this:\n",
    "\n",
    "$$P(H|D) = \\frac{P(H) ~ P(D|H)}{P(D)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this context, each term has a name:\n",
    "\n",
    "* $P(H)$ is the **prior probability** of the hypothesis, which represents how confident you are that $H$ is true prior to seeing the data,\n",
    "\n",
    "* $P(D|H)$ is the **likelihood** of the data, which is the probability of seeing $D$ if the hypothesis is true,\n",
    "\n",
    "* $P(D)$ is the **total probability of the data**, that is, the chance of seeing $D$ regardless of whether $H$ is true or not.\n",
    "\n",
    "* $P(H|D)$ is the **posterior probability** of the hypothesis, which indicates how confident you should be that $H$ is true after taking the data into account.\n",
    "\n",
    "An example will make all of this clearer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cookie problem\n",
    "\n",
    "Here's a problem I got from Wikipedia a long time ago, but now it's been edited away.\n",
    "\n",
    "> Suppose you have two bowls of cookies.  \n",
    ">\n",
    "> * Bowl 1 contains 30 vanilla and 10 chocolate cookies.  \n",
    ">\n",
    "> * Bowl 2 contains 20 vanilla and 20 chocolate cookies.\n",
    ">\n",
    "> You choose one of the bowls at random and, without looking into the bowl, choose one of the cookies at random.  It turns out to be a vanilla cookie.\n",
    ">\n",
    "> What is the chance that you chose Bowl 1?\n",
    "\n",
    "We'll assume that there was an equal chance of choosing either bowl and an equal chance of choosing any cookie in the bowl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can solve this problem using Bayes's Theorem.  First, I'll define $H$ and $D$:\n",
    "\n",
    "* $H_1$ is the hypothesis that the bowl you chose is Bowl 1.\n",
    "\n",
    "* $D$ is the datum that the cookie is vanilla (\"datum\" is the rarely-used singular form of \"data\").\n",
    "\n",
    "What we want is the posterior probability of $H_1$, which is $P(H_1|D)$.  It is not obvious how to compute it directly, but if we can figure out the terms on the right-hand side of Bayes's Theorem, we can get to it indirectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $P(H_1)$ is the prior probability of $H_1$, which is the probability of choosing Bowl 1 before we see the data.  If there was an equal chance of choosing either bowl, $P(H_1)$ is $1/2$.\n",
    "\n",
    "2. $P(D|H_1)$ is the likelihood of the data, which is the chance of getting a vanilla cookie if $H_1$ is true, in other words, the chance of getting a vanilla cookie from Bowl 1, which is $30/40$ or $3/4$.\n",
    "\n",
    "3. $P(D)$ is the total probability of the data, which is the chance of getting a vanilla cookie whether $H_1$ is true or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior and likelihood are relatively easy to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of the data is more difficult.\n",
    "To compute $P(D)$, I'll use [the law of total probability](https://en.wikipedia.org/wiki/Law_of_total_probability).\n",
    "\n",
    "Let's define $H_2$ to be the hypothesis that the bowl you chose is Bowl 2.\n",
    "\n",
    "We know that either $H_1$ or $H_2$ is true (and not both), so we can write:\n",
    "\n",
    "$$ P(D) = P(H_1) ~ P(D|H_1) + P(H_2) ~ P(D|H_2)$$\n",
    "\n",
    "Based on the statement of the problem, we have:\n",
    "\n",
    "* $P(H_1) = 1/2$\n",
    "\n",
    "* $P(D|H_1) = 3/4$\n",
    "\n",
    "* $P(H_2) = 1/2$\n",
    "\n",
    "* $P(D|H_2) = 1/2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the terms on the right-hand side, we can use Bayes's Theorem to combine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior probability is $0.6$, a little higher than the prior, which was $0.5$.  \n",
    "So the vanilla cookie makes us a little more certain that we chose Bowl 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bayes table\n",
    "\n",
    "Computing the total probability of the data is often the hardest part of the problem.\n",
    "Fortunately, there is another way to solve problems like this that makes it easier: the Bayes table.\n",
    "You can write a Bayes table on paper or use a spreadsheet, but in this notebook I'll use a Pandas DataFrame.\n",
    "\n",
    "As an example, I'll use a Bayes table to solve the cookie problem.  Here's an empty DataFrame with one row for each hypothesis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.DataFrame(index=['Bowl 1', 'Bowl 2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll add a column to represent the priors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "table['prior'] = 1/2, 1/2\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a column for the likelihoods:\n",
    "\n",
    "* The chance of getting a vanilla cookie from Bowl 1 is 3/4.\n",
    "\n",
    "* The chance of getting a vanilla cookie from Bowl 2 is 1/2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "table['likelihood'] = 3/4, 1/2\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The next step is similar to what we did with Bayes's Theorem; we multiply the priors by the likelihoods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "table['unnorm'] = table['prior'] * table['likelihood']\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each value in `unnorm` is the product of a prior and a likelihood.  \n",
    "\n",
    "* The first element is $P(H_1) ~ P(D|H_1)$.\n",
    "\n",
    "* The second element is $P(H_2) ~ P(D|H_2)$.\n",
    "\n",
    "According to the law of total probability, the sum of those terms is the probability of the data, $P(D)$:\n",
    "\n",
    "$$P(D) = P(H_1) ~ P(D|H_1) + P(H_2) ~ P(D|H_2)$$\n",
    "\n",
    "So we can compute $P(D)$ by adding up the elements of `unnorm`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_data = table['unnorm'].sum()\n",
    "prob_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we get 5/8, which is what we got by computing $P(D)$ explicitly.\n",
    "\n",
    "Now we divide by $P(D)$ to get the posterior probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "table['posterior'] = table['unnorm'] / prob_data\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior probability for Bowl 1 is 0.6, which is what we got using Bayes's Theorem.\n",
    "As a bonus, we also get the posterior probability of Bowl 2, which is 0.4.\n",
    "\n",
    "When we add up the unnormalized posteriors and divide through, we force the posteriors to add up to 1.  This process is called \"normalization\", which is why the total probability of the data is also called the \"[normalizing constant](https://en.wikipedia.org/wiki/Normalizing_constant#Bayes'_theorem)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def make_bayes_table(hypos, prior, likelihood):\n",
    "    \"\"\"Make a Bayes table.\n",
    "    \n",
    "    hypos: sequence of hypotheses\n",
    "    prior: prior probabilities\n",
    "    likelihood: sequence of likelihoods\n",
    "    \n",
    "    returns: DataFrame\n",
    "    \"\"\"\n",
    "    table = pd.DataFrame(index=hypos)\n",
    "    table['prior'] = prior\n",
    "    table['likelihood'] = likelihood\n",
    "    table['unnorm'] = table['prior'] * table['likelihood']\n",
    "    prob_data = table['unnorm'].sum()\n",
    "    table['posterior'] = table['unnorm'] / prob_data\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we can use this function to solve the cookie problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypos = 'Bowl 1', 'Bowl 2'\n",
    "prior = 1/2, 1/2\n",
    "likelihood = 3/4, 1/2\n",
    "\n",
    "make_bayes_table(hypos, prior, likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "What if we had chosen a chocolate cookie instead?  Use a Bayes table to compute the posterior probability of Bowl 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evidence\n",
    "\n",
    "In the previous example and exercise, notice a pattern:\n",
    "\n",
    "* A vanilla cookie is more likely if we chose Bowl 1, so getting a vanilla cookie makes Bowl 1 more likely.\n",
    "\n",
    "* A chocolate cookie is less likely if we chose Bowl 1, so getting a chocolate cookie makes Bowl 1 less likely.\n",
    "\n",
    "If data makes the probability of a hypothesis go up, we say that it is \"evidence in favor\" of the hypothesis.\n",
    "\n",
    "If data makes the probability of a hypothesis go down, it is \"evidence against\" the hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise \n",
    "\n",
    "One nice thing about the table method is that it works with more than two hypotheses.  As an example, let's do another version of the cookie problem.\n",
    "\n",
    "Suppose you have five bowls:\n",
    "\n",
    "* Bowl 0 contains no vanilla cookies.\n",
    "\n",
    "* Bowl 1 contains 25% vanilla cookies.\n",
    "\n",
    "* Bowl 2 contains 50% vanilla cookies.\n",
    "\n",
    "* Bowl 3 contains 75% vanilla cookies.\n",
    "\n",
    "* Bowl 4 contains 100% vanilla cookies.\n",
    "\n",
    "Now suppose we choose a bowl at random and then choose a cookie, and we get a vanilla cookie.  What is the posterior probability that we chose each bowl?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Exercise\n",
    "\n",
    "> Suppose you have two coins in a box.  One is a normal coin with heads on one side and tails on the other, and one is a trick coin with heads on both sides.\n",
    ">\n",
    "> You choose a coin at random and see that one of the sides is heads.  Is this data evidence in favor of, or against, the hypothesis that you chose the trick coin?\n",
    "\n",
    "See if you can figure out the answer before you read my solution.  I suggest these steps:\n",
    "\n",
    "1. First, state clearly what is the hypothesis and what is the data.\n",
    "\n",
    "2. Then think about the prior, the likelihood of the data, and the total probability of the data.\n",
    "\n",
    "3. Apply Bayes's Theorem or use a Bayes table to compute the posterior probability of the hypothesis.\n",
    "\n",
    "4. Use the result to answer the question as posed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
